---
layout: post
title: Discovering Neural Wirings
date: 2019-9-10
description: How we learn the connectivity of an Artificial Neural Network.
---
by _Mitchell Wortsman_, _Alvaro Herrasti_, _Sarah Pratt_, _Ali Farhadi_ and _Mohammad Rastegari_ from _The Allen Institute for Artificial Intelligence_, _University of Washington_, and _XNOR.AI_.

In this post we discuss the most interesting bits from our recent paper on [Discovering Neural Wirings](https://arxiv.org/abs/1906.00586) (to appear at NeurIPS 2019). Primarily, the connectivity patterns of Artificial Neural Networks (ANNs) are manually defined or largely constrained. In contrast, we relax the typical notion of layers to allow for a much larger space of possible wirings. The wiring of our ANN is not fixed during training -- as we learn the network parameters we also learn the connectivity. Move the slider below to see how the wiring changes when a small network is trained on MNIST for a few epochs.

{% include dnw-demo.html %}

### Why Wiring?

Before the advent of modern ANNs, researchers would manually engineer good _features_ (high dimensional vector representations). Good features may now be _learned_ with ANNs, but the _architecture_ of the ANN must be specified instead. Accordingly, a myriad of recent work in [Neural Architecture Search (NAS)](https://arxiv.org/abs/1611.01578) has focused on _learning_ the architecture of an ANN.
However, NAS still searches among a set of manually designed building blocks and so ANN connectivity remains largely constrained. In contrast, [RandWire](https://arxiv.org/pdf/1904.01569.pdf) explores a diverse set of connectivity patterns by considering ANNs which are randomly wired.
Although randomly wired ANNs are competitive with NAS, their connectivity is fixed during training.

We propose a method for jointly learning the parameters and wiring of an ANN during training. We demonstrate that our method of _Discovering Neural Wirings (DNW)_ outperforms many manually-designed and randomly wired ANNs.

ANNs are inspired by the biological neural networks of the animal brain. Despite the countless fundamental differences between these two systems, a biological inspiration may still prove useful. A recent Nature Communications article (aptly titled [A critique of pure learning and what artificial neural networks can learn from animal brains](https://www.nature.com/articles/s41467-019-11786-6)) argues that the connectivity of an animal brain enables rapid learning. Accordingly, the article suggests _"wiring topology and network architecture as a target for optimization in artificial systems."_ We hope that this work provides a useful step in this direction.

Concurrent work on [Weight Agnostic Neural Networks](https://weightagnostic.github.io/) also emphasizes the importance of ANN wiring. They demonstrate that a given wiring for an ANN can effectively solve some simple tasks without any training -- the solution is encoded in the connectivity.

### _Static Neural Graphs (SNGs)_: A Convenient Abstraction for a Feed Forward ANN

We now describe a convenient abstraction for a feed-forward ANN -- a _Static Neural Graph (SNG)_. Our goal is then to learn the optimal _edge set_ of the SNG. Although this setting should feel familiar, we skim over some low level details below and invite you to reference the [paper](https://arxiv.org/abs/1906.00586).

An _SNG_ is a directed acyclic graph $$G$$ which consists of nodes $$\mathcal{V}$$ and edges $$\mathcal{E}$$. Additionally, each node $$v$$ has output $$\mathcal{Z}_v$$ and input $$\mathcal{I}_v$$. Input data $$X$$ flows into the network through a designated set of nodes $$\mathcal{V}_0$$, and the input to node $$v \in \mathcal{V} \setminus \mathcal{V}_0$$ is a weighted sum of the parent's outputs

$$
\mathcal{I}_v = \sum_{(u,v) \in \mathcal{E}} w_{u,v}\mathcal{Z}_u
$$

The output of each node is then computed via a parameterized function

$$
\mathcal{Z}_v = f_{\theta_v}\left(\mathcal{I}_v\right)
$$

where $$w_{u,v}$$ and $$\theta_v$$ are the learned network parameters. The output of the network is then computed via a designated set of nodes $$\mathcal{V}_E$$.

In this work we are designing models for Computer Vision, and so each node resembles a single _channel_. Accordingly, $$f_{\theta_v}$$ performs a convolution over a 2-dimensional matrix (followed by [Batch Norm](https://arxiv.org/abs/1502.03167) and ReLU).

### An Algorithm for Discovering Neural Wirings

How do we learn the optimal edge set $$\mathcal{E}$$ during training? We follow recent work (such as [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)) and attribute the "importance" of a parameter to its magnitude. Accordingly, the link between node $$u$$ and $$v$$ is considered important if $$ \lvert w_{u,v} \rvert \gg 0$$.

The edge set is chosen by taking the edges with the highest magnitude weight:

$$\mathcal{E} = \{ (u,v) \ : \ \lvert w_{u,v} \rvert > \tau \textrm{ and } u \lt v \}$$

where $$\tau$$ is chosen so that there are exactly $$k$$ edges and $$u \lt v$$ ensures that the graph is acyclic.


### Lottery Tickets? Sparse Networks?

{% include sparse-graph.html %}

### Appendix A: Proofs

### Appendix B: _Dynamic Neural Graphs (DNGs)_

### Appendix C: Efficient _SNGs_
