---
layout: post
title: Discovering Neural Wirings
date: 2019-9-19
description: How we learn the connectivity of an Artificial Neural Network.
---
blog by _Mitchell Wortsman_, _Alvaro Herrasti_, _Sarah Pratt_, _Ali Farhadi_ and _Mohammad Rastegari_ from _The Allen Institute for Artificial Intelligence_, _University of Washington_, and _XNOR.AI_.

In this post we discuss the most interesting contributions from our recent paper on [**Discovering Neural Wirings**](https://arxiv.org/abs/1906.00586)
(to appear at NeurIPS 2019). Traditionally, the connectivity patterns of Artificial Neural Networks (ANNs)
are manually defined or largely constrained. In contrast, we relax the typical notion of layers to
allow for a much larger space of possible wirings. The wiring of our ANN is not fixed during
training -- as we learn the network parameters we also learn the connectivity.


In our pursuit
we also arrive at the following [conclusion](#sparse-networks-lottery-tickets-overparameterization):
it is possible to train a model that is small during inference but still overparameterized during training.
By applying our method to discover sparse ANNs we bridge the gap between
[Neural Architecture Search](https://arxiv.org/abs/1611.01578) and [Sparse Learning](https://arxiv.org/pdf/1907.04840v2.pdf).

Move the slider below to see how the wiring changes when a small network is trained on MNIST for a few epochs.

{% include dnw-demo.html %}

### Why Wiring?

Before the advent of modern ANNs, researchers would manually engineer good _features_ (high dimensional vector representations). Good features may now be _learned_ with ANNs, but the _architecture_ of the ANN must be specified instead. Accordingly, a myriad of recent work in [Neural Architecture Search (NAS)](https://arxiv.org/abs/1611.01578) has focused on _learning_ the architecture of an ANN.
However, NAS still searches among a set of manually designed building blocks and so ANN connectivity remains largely constrained. In contrast, [RandWire](https://arxiv.org/pdf/1904.01569.pdf) explores a diverse set of connectivity patterns by considering ANNs which are randomly wired.
Although randomly wired ANNs are competitive with NAS, their connectivity is fixed during training.

We propose a method for jointly learning the parameters and wiring of an ANN during training. We demonstrate that our method of _Discovering Neural Wirings (DNW)_ outperforms many manually-designed and randomly wired ANNs.

ANNs are inspired by the biological neural networks of the animal brain. Despite the countless fundamental differences between these two systems, a biological inspiration may still prove useful. A recent Nature Communications article (aptly titled [A critique of pure learning and what artificial neural networks can learn from animal brains](https://www.nature.com/articles/s41467-019-11786-6)) argues that the connectivity of an animal brain enables rapid learning. Accordingly, the article suggests _"wiring topology and network architecture as a target for optimization in artificial systems."_ We hope that this work provides a useful step in this direction.

Concurrent work on [Weight Agnostic Neural Networks](https://weightagnostic.github.io/) also emphasizes the importance of ANN wiring. They demonstrate that a given wiring for an ANN can effectively solve some simple tasks without any training -- the solution is encoded in the connectivity.

### _Static Neural Graphs (SNGs)_: A Convenient Abstraction for a Feed Forward ANN

We now describe a convenient abstraction for a feed-forward ANN -- a _Static Neural Graph (SNG)_. Our goal is then to learn the optimal _edge set_ of the SNG.
We skim over some low level details below and invite you to reference the [paper](https://arxiv.org/abs/1906.00586),
though this abstraction should feel familiar.

An _SNG_ is a directed acyclic graph $$G$$ which consists of nodes $$\mathcal{V}$$ and edges $$\mathcal{E}$$. Additionally, each node $$v$$ has output $$\mathcal{Z}_v$$ and input $$\mathcal{I}_v$$. Input data $$X$$ flows into the network through a designated set of nodes $$\mathcal{V}_0$$, and the input to node $$v \in \mathcal{V} \setminus \mathcal{V}_0$$ is a weighted sum of the parent's outputs

$$
\mathcal{I}_v = \sum_{(u,v) \in \mathcal{E}} w_{u,v}\mathcal{Z}_u
$$

The output of each node is computed via a parameterized function

$$
\mathcal{Z}_v = f_{\theta_v}\left(\mathcal{I}_v\right)
$$

where $$w_{u,v}$$ and $$\theta_v$$ are the learnable network parameters. The output of the network is then computed via a designated set of nodes $$\mathcal{V}_E$$.

In this work we are designing models for Computer Vision, and so each node resembles a single _channel_ (as illustrated below).
Accordingly, $$f_{\theta_v}$$ performs a convolution over a 2-dimensional matrix (followed by [BatchNorm](https://arxiv.org/abs/1502.03167) and ReLU).

<div style="text-align:center"><img src="/assets/img/dnw_fig.png" width="200"/></div>

### An Algorithm for Discovering Neural Wirings

How do we learn the optimal edge set $$\mathcal{E}$$ during training? We follow recent work (such as [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)) and attribute the "importance" of a parameter to its magnitude. Accordingly, the link between node $$u$$ and $$v$$ is considered _important_ if $$ \lvert w_{u,v} \rvert \gg 0$$.

At each training iteration the edge set is chosen by taking the highest magnitude weights:

$$\mathcal{E} = \{ (u,v) \ : \ \lvert w_{u,v} \rvert > \tau \textrm{ and } u \lt v \}$$

where $$\tau$$ is chosen so that there are exactly $$k$$ edges and $$u \lt v$$ ensures that the graph is acyclic.

All that remains is to choose a weight update for $$w_{u,v}$$. Recall that most use [backpropogation](https://www.nature.com/articles/323533a0)
where gradients from a loss term $$\mathcal{L}$$ are passed backwards through the network. Using the chain rule gradients may be computed with respect to each network parameter.
The parameters are then often updated via _stochastic gradient descent_ with some learning rate $$\alpha$$. Conveniently, standard backprop will automatically compute the quantity

$$g_v = - \alpha \frac{\partial \mathcal{L}}{\partial \mathcal{I}_v}$$

Informally, the quantity $$g_v$$ describes how the network _wants_ $$\mathcal{I}_v$$ to change so that the loss will decrease.
Our rule is therefore to strengthen the connection between $$u$$ and $$v$$ when $$\mathcal{Z}_u$$ aligns with $$g_v$$.
If node $$u$$ can take $$\mathcal{I}_v$$ where the loss _wants_ it to go, we should add an edge from $$u$$ to $$v$$. We therefore modify $$w_{u,v}$$ via our update rule

$$w_{u,v} \gets w_{u,v} + \left\langle \mathcal{Z}_u, g_v \right\rangle$$

where $$\langle \cdot, \cdot \rangle$$ denotes an inner product (these quantities are implicitly treated as vectors).

In practice $$w_{u,v}$$ changes only a small amount at each training iteration.
However, if $$\mathcal{Z}_u$$ consistently aligns with $$g_v$$ then $$\lvert w_{u,v} \rvert$$ will strengthen to a point where edge $$(u,v)$$ replaces a weaker edge. We show in our [paper](https://arxiv.org/abs/1906.00586) that when swapping does occur, it is beneficial under some assumptions.

When training, the rest of the network is updated via backprop as usual. In fact, you may notice that the update rule exactly resembles SGD when $$(u,v) \in \mathcal{E}$$. And so the algorithm may be interpreted equivalently as allowing the gradient to flow to, but not through, a set of "potential" edges. In practice we include a momentum and weight decay term as is standard practice with SGD (weight decay should eventually remove dead ends).

### Putting it Together
<img src="/assets/gif/dnw.gif" width="600"/>

### Wirings at Scale

We employ the following two strategies for discovering wirings at scale:

1. We chain together multiple graphs, $$G_1, G_2, ..., G_n$$ where the output of $$G_i$$ is the input to $$G_{i+1}$$. The _input nodes_ perform a strided convolution and the spatial resolution is fixed throughout the remaining nodes in the graph.

2. The depth of the graph is limited to be $$\ell$$ by partitioning the nodes into blocks $$\mathcal{B}_1,...,\mathcal{B}_\ell$$. We then only allow connections between nodes $$u \in \mathcal{B}_i$$ and $$v \in \mathcal{B}_j$$ if $$i \lt j$$.

For an even comparison, we consider the exact same structure and number of edges as MobileNet V1 if it were interpreted as a chain of graphs. By learning the connectivity we boost the ImageNet accuracy by ~10% in the low compute setting.

| Model | ImageNet Top-1 Accuracy |
| :-------------: | :-------------: |
| Original MobileNet V1 (x 0.25)  | 50.6 %  |
| Random Graph MobileNet V1 (x 0.225) | 53.3 % |
| Discovered MobileNet V1 (x 0.225)| 60.9 % |


### Sparse Networks? Lottery Tickets? Overparameterization?

The past few years have witnessed illuminating work in the field of sparse ANNs.
In [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635),
Frankle and Carbin demonstrate that dense ANNs contain
subnetworks that can be effectively trained in isolation. However,
their process for uncovering these so-called _winning tickets_ is
expensive as it first requires a dense network to be trained.
In [Sparse Networks from Scratch](https://arxiv.org/pdf/1907.04840.pdf),
Dettmers and Zettlemoyer introduce _sparse learning_ -- training ANNs only once while maintaining sparse weights throughout.

Our work aims to unify the problem of neural architecture search
with sparse learning. As NAS becomes less restrictive and more fine grained,
finding a good architecture is akin to finding a sparse subnetwork of the complete graph.

Accordingly, we may use our algorithm for Discovering Neural Wirings and apply it to
the task of training other sparse ANNs. Our method requires no fine-tuning
or retraining to discover a sparse subnetwork. This perspective was guided
by Dettmers and Zettelmoyer, though we would like to highlight some differences.
Their work enables faster training, which we do not achieve as our
backwards pass is still dense. Moreover, their work allows a smart
redistribution of parameters across layers which we do not attempt. Finally, their
training is more memory efficient as they actually send unused
weights to zero while we continue to update them in the backwards pass.

We leave the biases and batchnorm dense and use
a [tuned ResNet-50](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/RN50v1.5).
This mirrors the experimental set-up from Appendix C of [Sparse Networks from Scratch](https://arxiv.org/pdf/1907.04840v2.pdf).
The figure below illustrates how top-1 accuracy varies with the sparsity
(of the convolutional filters and linear weight -- a sparsity of 0% corresponds to
the dense network). The figure shows an alternative setting where the first convolutional
layer (with &lt; 10k parameters $$\approx$$ 0.04% of the total network) is left dense.

{% include sparse-graph.html %}

To generate the figure above we consider only multiples of 10% and the rest is
interpolated. All models and numbers will soon be found on our [Github](https://github.com/allenai/dnw)
though we provide relevant **ImageNet Top-1 Accuracy** metrics for ResNet-50 below.

| Model | 10% of Weights | 20% of Weights |
| :-------------: | :-------------: | :-------------: |
| All Layers Sparse (ours)  | 74.0 %  | 76.2 %  |
| First Layer Dense (ours) | 75.0 %  | 76.6 % |
|[Sparse Networks from Scratch](https://arxiv.org/pdf/1907.04840v2.pdf)| 72.9 % | 74.9 % |

We would like to highlight an interesting conclusion we may draw from this result:
**It is possible to realize the benefits of overparameterization during training even when the resulting model is not itself overparameterized.** In other words, it is possible to train a model that is **small** during inference but **still overparameterized** during training. Though the we only ever use a small percentage
of the weights during the forwards pass, our network has the _same odds at winning the [initialization lottery](https://arxiv.org/pdf/1907.04840v2.pdf)_
as a much larger network.

The implementation for training sparse ANNs with our algorithm is quite simple (we implicitly treat
each parameter as an edge).
All convolutions are replaced with the following [pytorch](https://pytorch.org/) code:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import autograd


class ChooseEdges(autograd.Function):
    @staticmethod
    def forward(ctx, weight, prune_rate):
        output = weight.clone()
        _, idx = weight.flatten().abs().sort()
        p = int(prune_rate * weight.numel())
        # flat_oup and output access the same memory.
        flat_oup = output.flatten()
        flat_oup[idx[:p]] = 0
        return output

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output, None


class SparseConv(nn.Conv2d):
    def set_prune_rate(self, prune_rate):
        self.prune_rate = prune_rate

    def forward(self, x):
        w = ChooseEdges.apply(self.weight, self.prune_rate)
        x = F.conv2d(
            x, w,
            self.bias, self.stride, self.padding, self.dilation, self.groups
        )
        return x
```

The figure below illustrates how this code works.

<div style="text-align:center"><img src="/assets/img/dnw_code_fig.png" width="600"/></div>

### Discovering Neural Wirings for _Dynamic Neural Graphs (DNGs)_

We may also apply our algorithm to _processes_ on graphs, where nodes receive
input and produce output at all times $$t$$ (and the graph is not restricted to be a DAG).
In the _discrete time_ setting, we consider times
$$t \in \{0, 1,..., T\}$$ and let the input and output vary with $$t$$
(we skim over some low level details -- i.e. the initial conditions -- and invite
you to reference the [paper](https://arxiv.org/abs/1906.00586)). The state of
node $$v$$ at time $$t \gt 0$$ is then given by

$$
\mathcal{Z}_v(t) = f_{\theta_v}\left(\sum_{(u,v) \in \mathcal{E}} w_{u,v}Z_u(t-1)\right)
$$

We may also consider the setting where $$t$$ takes on a _continuous_ range of values (as in [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366)). The ANN evolves according to the following dynamics:

$$
{d \mathcal{Z}_v(t) \over d t} = f_{\theta_v}\left(\sum_{(u,v) \in \mathcal{E}} w_{u,v}Z_u(t)\right)
$$

We apply our algorithm for Discovering Neural Wirings to a tiny (41k parameter)
classifier in both the _static_ and _dynamic_ setting.

| Model  | Accuracy (CIFAR-10) |
| :-------------: | :-------------: |
Static (Random Graph) | 76.1 $$\pm$$ 0.5|
Static (ours) | 80.9 $$\pm$$ 0.6 |
Discrete Time (Random Graph) | 77.3$$\pm$$ 0.7|
Discrete Time (ours) | 82.3$$\pm$$ 0.6 |
Continuous (Random Graph) | 78.5 $$\pm$$ 1.2  |
Continuous (ours) | 83.1 $$\pm$$ 0.3 |

<div style="text-align:center"><img src="/assets/img/dnw_dynamic_fig.png" width="250"/></div>
### Citing

If you found this work to be helpful please consider citing:

```
@article{Wortsman2019DiscoveringNW,
  title={Discovering Neural Wirings},
  author={Mitchell Wortsman and Ali Farhadi and Mohammad Rastegari},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.00586}
}
```

### Acknowledgements

We sincerely thank Tim Dettmers for his assistance and guidance in the experiments regarding sparse networks.

### Comments and FAQ

To comment raise an issue on our github repo [here](https://github.com/allenai/dnw).
