---
layout: post
title: Discovering Neural Wirings
date: 2019-9-10
description: How we learn the connectivity of an Artificial Neural Network.
---
by _Mitchell Wortsman_, _Alvaro Herrasti_, _Sarah Pratt_, _Ali Farhadi_ and _Mohammad Rastegari_ from _The Allen Institute for Artificial Intelligence_, _University of Washington_, and _XNOR.AI_.

In this post we discuss the most interesting bits from our recent paper on [Discovering Neural Wirings](https://arxiv.org/abs/1906.00586) (to appear at NeurIPS 2019). Primarily, the connectivity patterns of Artificial Neural Networks (ANNs) are manually defined or largely constrained. In contrast, we relax the typical notion of layers to allow for a much larger space of possible wirings. The wiring of our ANN is not fixed during training -- as we learn the network parameters we also learn the connectivity. Move the slider below to see how the wiring changes when a small network is trained on MNIST for a few epochs.

{% include dnw-demo.html %}

### Why Wiring?

Before the advent of modern ANNs, researchers would manually engineer good _features_ (high dimensional vector representations). Good features may now be _learned_ with ANNs, but the _architecture_ of the ANN must be specified instead. Accordingly, a myriad of recent work in [Neural Architecture Search (NAS)](https://arxiv.org/abs/1611.01578) has focused on _learning_ the architecture of an ANN.
However, NAS still searches among a set of manually designed building blocks and so ANN connectivity remains largely constrained. In contrast, [RandWire](https://arxiv.org/pdf/1904.01569.pdf) explores a diverse set of connectivity patterns by considering ANNs which are randomly wired.
Although randomly wired ANNs are competitive with NAS, their connectivity is fixed during training.

We propose a method for jointly learning the parameters and wiring of an ANN during training. We demonstrate that our method of _Discovering Neural Wirings (DNW)_ outperforms many manually-designed and randomly wired ANNs.

ANNs are inspired by the biological neural networks of the animal brain. Despite the countless fundamental differences between these two systems, a biological inspiration may still prove useful. A recent Nature Communications article (aptly titled [A critique of pure learning and what artificial neural networks can learn from animal brains](https://www.nature.com/articles/s41467-019-11786-6)) argues that the connectivity of an animal brain enables rapid learning. Accordingly, the article suggests _"wiring topology and network architecture as a target for optimization in artificial systems."_ We hope that this work provides a useful step in this direction.

Concurrent work on [Weight Agnostic Neural Networks](https://weightagnostic.github.io/) also emphasizes the importance of ANN wiring. They demonstrate that a given wiring for an ANN can effectively solve some simple tasks without any training -- the solution is encoded in the connectivity.

### _Static Neural Graphs (SNGs)_: A Convenient Abstraction for a Feed Forward ANN

We now describe a convenient abstraction for a feed-forward ANN -- a _Static Neural Graph (SNG)_. Our goal is then to learn the optimal _edge set_ of the SNG. Although this setting should feel familiar, we skim over some low level details below and invite you to reference the [paper](https://arxiv.org/abs/1906.00586).

An _SNG_ is a directed acyclic graph $$G$$ which consists of nodes $$\mathcal{V}$$ and edges $$\mathcal{E}$$. Additionally, each node $$v$$ has output $$\mathcal{Z}_v$$ and input $$\mathcal{I}_v$$. Input data $$X$$ flows into the network through a designated set of nodes $$\mathcal{V}_0$$, and the input to node $$v \in \mathcal{V} \setminus \mathcal{V}_0$$ is a weighted sum of the parent's outputs

$$
\mathcal{I}_v = \sum_{(u,v) \in \mathcal{E}} w_{u,v}\mathcal{Z}_u
$$

The output of each node is then computed via a parameterized function

$$
\mathcal{Z}_v = f_{\theta_v}\left(\mathcal{I}_v\right)
$$

where $$w_{u,v}$$ and $$\theta_v$$ are the learned network parameters. The output of the network is then computed via a designated set of nodes $$\mathcal{V}_E$$.

In this work we are designing models for Computer Vision, and so each node resembles a single _channel_. Accordingly, $$f_{\theta_v}$$ performs a convolution over a 2-dimensional matrix (followed by [BatchNorm](https://arxiv.org/abs/1502.03167) and ReLU).

### An Algorithm for Discovering Neural Wirings

How do we learn the optimal edge set $$\mathcal{E}$$ during training? We follow recent work (such as [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)) and attribute the "importance" of a parameter to its magnitude. Accordingly, the link between node $$u$$ and $$v$$ is considered _important_ if $$ \lvert w_{u,v} \rvert \gg 0$$.

At each training iteration the edge set is chosen by taking the edges with the highest magnitude weight:

$$\mathcal{E} = \{ (u,v) \ : \ \lvert w_{u,v} \rvert > \tau \textrm{ and } u \lt v \}$$

where $$\tau$$ is chosen so that there are exactly $$k$$ edges and $$u \lt v$$ ensures that the graph is acyclic.

All that remains is to choose a weight update rule for the edge weights. Recall that we often use [backpropogation](https://www.nature.com/articles/323533a0). In short, gradients from a loss term $$\mathcal{L}$$ are passed backwards though the network to compute gradients with respect to the network parameters. The parameters are often then updated via _stochastic gradient descent_ with some learning rate $$\alpha$$. Conveniently, standard backprop will automatically compute the quantity

$$g_v = - \alpha \frac{\partial \mathcal{L}}{\partial \mathcal{I}_v}$$

Informally, the quantity $$g_v$$ describes how the network wants $$\mathcal{I}_v$$ to change so that the loss will decrease. Our rule is therefore to strengthen the connection between $$u$$ and $$v$$ when $$\mathcal{Z}_u$$ aligns with $$g_v$$. Informally, if node $$u$$ can take $$\mathcal{I}_v$$ where the loss _wants_ it to go, we should add an edge from $$u$$ to $$v$$. We therefore modify $$w_{u,v}$$ via our update rule

$$w_{u,v} \gets w_{u,v} + \left\langle \mathcal{Z}_u, g_v \right\rangle$$

where $$\langle \cdot, \cdot \rangle$$ denotes an inner product (these quantities are implicitly treated as vectors).

In practice $$w_{u,v}$$ changes only a small amount at each training iteration. However, if $$\mathcal{Z}_u$$ consistently aligns with $$g_v$$ then $$\lvert w_{u,v} \rvert$$ will strengthen to a point where edge $$(u,v)$$ replaces a weaker edge. We show [below](#appendix-a-proofs) that when swapping does occur, it is beneficial under some assumptions.

When training, the rest of the network is updated via backprop as usual. In fact, you may notice that the update rule exactly resembles SGD when $$(u,v) \in \mathcal{E}$$. And so the algorithm may be interpreted equivalently as allowing the gradient to flow to, but not through, a set of "potential" edges. In practice we include a momentum and weight decay term as is standard practice with SGD (weight decay should eventually remove dead ends). See the [paper](https://arxiv.org/abs/1906.00586) for yet another interpretation of the update rule as performing an approximation in the backwards pass.

### Wirings at Scale

We employ the following two strategies for discovering wirings at scale:

1. We chain together multiple graphs, $$G_1, G_2, ..., G_n$$ where the output of $$G_i$$ is the input to $$G_{i+1}$$. The _input nodes_ perform a strided convolution and the spatial resolution is fixed throughout the remaining nodes in the graph.

2. The depth of the graph is limited to be $$\ell$$ by partitioning the nodes into blocks $$\mathcal{B}_1,...,\mathcal{B}_\ell$$. We then only allow connections between nodes $$u \in \mathcal{B}_i$$ and $$v \in \mathcal{B}_j$$ if $$i \lt j$$.

For an even comparison, we consider the exact same structure and number of edges as MobileNet V1 if it were interpreted as a chain of graphs. By learning the connectivity we boost the ImageNet accuracy by ~10% in the low compute setting.

| Model | ImageNet Top-1 Accuracy |
| ------------- |-------------|
| Original MobileNet V1 (x 0.25)  | 50.6 %  |
| Random Graph MobileNet V1 (x 0.225) | 53.3 % |
| Discovered MobileNet V1 (x 0.225)| 60.9 % |


### Sparse Networks? Lottery Tickets?

A few recent papers have challenged our understanding of sparse neural networks.
In [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635),
Frankle and Carbin demonstrate that dense ANNs contain
subnetworks that can be effectively trained in isolation. However,
their process for uncovering these so-called _winning tickets_ is
expensive as it first requires a dense network to be trained.
In [Sparse Networks from Scratch](https://arxiv.org/pdf/1907.04840.pdf),
Dettmers and Zettlemoyer show that ANNs may be trained while maintaining sparse weights.

This work has encouraged us to employ our algorithm for 

{% include sparse-graph.html %}


### Appendix A: Proofs

### Appendix B: _Dynamic Neural Graphs (DNGs)_

### Appendix C: FAQ
