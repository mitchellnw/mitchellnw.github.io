---
layout: post
title: Supermasks in Superposition
date: 2020-6-23
description: Using subnetworks to learn thousands of tasks without forgetting.
---
Learning many different tasks sequentially remains a challenge for Artificial Neural Networks (ANNs).
When learning a new task, networks tend to [*catastrophically forget*](https://arxiv.org/abs/1612.00796)
information from previous tasks. In this post we discuss Supermasks in Superposition (SupSup),
a flexible model capable of learning thousands of tasks without forgetting (and without access to task identity).

SupSup leverages the expressive power of neural network connectivity: an exciting line of recent work ([Zhou *et al.*](https://eng.uber.com/deconstructing-lottery-tickets/), [Ramanujan *et al.*](https://arxiv.org/abs/1911.13299))
demonstrates that even when the weights of an ANN are random and fixed, learning can still occur by modifying the network connectivity.
Instead of learning the weights, or [jointly learning the weights and connectivity](https://mitchellnw.github.io/blog/2019/dnw/),
good performance can be achieved by choosing whether each edge in the the network is *on* or *off*. [Later in this post](#supermasks) we detail
fast algorithms for finding these so-called **supermasks** (a binary mask which specifies if each edge is on or off, exposing a subnetwork which achieves good performance).

SupSup uses a neural network with weights that remain fixed and random. For each new task $$i$$ SupSup finds a **supermask** (subnetwork)
which achieves good performance on task $$i$$. The underlying weights are not modified and so forgetting does not occur. Moreover,
the underlying weights have minimal storage cost as you only need to save the random seed.

<div style="text-align:center"><img src="/assets/img/supsup_training.png" width="500"/></div>

When the task identity of data is given during inference, the corresponding supermask can then be used
(similar to [Mallya *et al.*](https://arxiv.org/abs/1801.06519) but without the need for a pretrained backbone).

SupSup may also be used even when task identity is unknown during inference (*i.e.* the model does not have access to which
task it is currently evaluating on). SupSup does so by inferring task identity and using the corresponding supermask (subnetwork).

The supermask trained on task $$i$$ should be [confident](https://arxiv.org/abs/1610.02136) when given data from task $$i$$.
Therefore, SupSup infers task identity by finding the supermask which produces the lowest entropy (highest confidence) outputs.

<div style="text-align:center"><img src="/assets/img/supsup_inference.png" width="500"/></div>

As the number of tasks grows large (> 1000), it would be impractical to iterate through each supermask and check the entropy
of the output distribution. Therefore, we consider a network with all supermasks in weighted 
**superposition**---mask $$i$$ is weighted by $$\alpha_i$$ (where each $$\alpha_i > 0$$ and $$\sum_i \alpha_i = 1$$ and
initially each $$\alpha_i = 1/k$$ where $$k$$ is the number of tasks learned so far).

### Continual Learning Scenarios

### Fantastic Supermasks and How to Find Them

### Broader Impact

### Citing

If you found this work to be helpful please consider citing:

TODO

### Acknowledgements

### Comments and FAQ

To comment raise an issue on our github repo TODO.
